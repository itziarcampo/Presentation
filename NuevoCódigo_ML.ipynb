{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfd045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your mutation:K4N\n",
      "Benign mutation\n",
      "Success rate: LOW\n"
     ]
    }
   ],
   "source": [
    "# Sólo código necesario\n",
    "# Este es el código principal que se anclará a la web. Además de este, existen otros dos scripts auxiliares uno con funciones\n",
    "# relativas al funcionamiento de este programa y otro que sirve para hacer la traduccion de adn a proteina.\n",
    "\n",
    "\n",
    "# Se importan las librerías y paquetes necesarios para el algoritmo.\n",
    "import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sys import exit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "\n",
    "#Carga de base de datos que sirve para entrenar al algoritmo\n",
    "wdf = pd.read_csv('corrected_FINAL_DDBB.csv', header = \"infer\")\n",
    "\n",
    "#Se realizan una serie de acciones que permiten acondicionar la base de datos para los propósitos de este algoritmo\n",
    "\n",
    "x1=list(wdf.loc[(wdf[\"label\"] == 1)][\"residue_conserv\"])\n",
    "l = []\n",
    "\n",
    "for i in x1:\n",
    "    l.append(i)\n",
    "    \n",
    "# Se localizan los outliers\n",
    "new_l = sorted(l)[9:]\n",
    "\n",
    "# Se eliminan los outliers de los datos originales.\n",
    "wdf = wdf.drop(wdf.loc[(wdf[\"label\"] == 1) & (wdf[\"residue_conserv\"] <= 0.6197)].index)\n",
    "\n",
    "# Se buscan valores duplicados\n",
    "wdf[wdf[\"mutation\"].duplicated()]\n",
    "\n",
    "# Se buscan datos faltantes\n",
    "wdf.isnull().sum()\n",
    "\n",
    "\n",
    "# Los datos vienen etiquetados en dos clases, 0 y 1. Se dividen según a qué clase pertenezcan. La clase 0 pertenece a las\n",
    "# mutaciones benignas y la clase 1 a las patógenas.\n",
    "y_be = (wdf.values[:,-1] == 0)\n",
    "y_pa = (wdf.values[:,-1] == 1)\n",
    "\n",
    "# En Machine Learning existen dos tipos de datos, los datos de entrenamiento que sirven para entrenar el algoritmo y los datos\n",
    "# de testeo que sirven para ver si el algoritmo ha aprendido correctamente y cumple con el objetivo esperado.\n",
    "# Es por ello que se tienen que dividir los datos de entrada en dos conjuntos, 'training set' y 'test set'. En este caso,\n",
    "# el 75% de los datos corresponderá al 'training set' y el 25% restante al 'test set'.\n",
    "\n",
    "X = wdf.values[:,2:-1]\n",
    "y = wdf.values[:,-1].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1012)\n",
    "\n",
    "# Se crean dos DataFrame, para el conjunto de datos de entrenamiento. En uno se guardan los datos y variables iniciales\n",
    "# (X_train_dfos), y en el otro la respuesta correspondiente a esos datos (y_train_dfos). El algoritmo deberá aprender la\n",
    "# relación entre las variables y la respuesta para poder predecir sobre datos cuya respuesta es desconocida.\n",
    "\n",
    "X_train_dfos = pd.DataFrame(X_train, columns = ['initial_aa', \n",
    "                                                'final_aa', \n",
    "                                                'topological_domain', \n",
    "                                                'functional_domain', \n",
    "                                                'd_size',\n",
    "                                                'd_hf',\n",
    "                                                'd_vol',\n",
    "                                                'd_msa',\n",
    "                                                'd_charge', \n",
    "                                                'd_pol', \n",
    "                                                'd_aro', \n",
    "                                                'residue_conserv',\n",
    "                                                'secondary_str',\n",
    "                                                'pLDDT',\n",
    "                                                'str_pos',\n",
    "                                                'MTR'])\n",
    "\n",
    "y_train_dfos = pd.DataFrame(y_train, columns = ['label'])\n",
    "\n",
    "# Dentro de los datos de entrenamiento, existen dos categorías, los datos de etiqueta 0 y de etiqueta 1. Los datos están\n",
    "# descompensados, hay muchos mas datos de la clase 1 que de la 0. Hay que balancearlos, para ello se realiza un oversampling,\n",
    "# que consiste en producir muestras aleatorias pertenecientes a la clase 0 para que haya más y el algoritmo pueda aprender bien.\n",
    "\n",
    "# Se unen todos los datos de entrenamiento en un único DataFrame\n",
    "dfos = pd.concat([X_train_dfos, y_train_dfos], axis=1)\n",
    "\n",
    "# Se separan la clase mayritaria (la del 1) y la minoritaria (la del 0).\n",
    "df_majority = dfos[dfos.label==1]\n",
    "df_minority = dfos[dfos.label==0]\n",
    "\n",
    "# Se realizan las nuevas muestras de la clase minoritaria\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority)//2,    \n",
    "                                 random_state=0)   # reproducible results\n",
    " \n",
    "# Se unen la clase mayoritaria y la nueva clase minoritaria en un mismo DataFrame\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Se vuelve a separar este nuevo conjunto de datos, en variables y resultados\n",
    "X_train_os = df_upsampled.values[:,:-1]\n",
    "y_train_os = df_upsampled.values[:,-1].astype(\"int\")\n",
    "\n",
    "# Existen una serie de variables que son de tipo cualitativ, es decir que no son numéricas. Para poder tratarlas computacional-\n",
    "# mente hace falta escribirlas de forma numérica. Para ello se usan una serie de algoritmos que sirven para codificar estos\n",
    "# datos.\n",
    "\n",
    "X_train_enc, X_test_enc, X_train_df, X_test_df = utils.categorical_encoding(X_train_os, X_test)\n",
    "X_train_df.columns \n",
    "\n",
    "# Hasta ahora solo se ha llevado a cabo el tratamiento de los datos. Ahora los datos ya están listos para ser procesados por el\n",
    "# algoritmo. En este modelo se utiliza un algoritmo 'ensemble', que combina tres algoritmos de clasificación distintos y \n",
    "# después predice la respuesta dada por la mayoría de ellos. En este algoritmo se utiliza el algoritmo 'Voting Classifier',\n",
    "# con el modelo de voto 'Soft Voting'. Dentro de este se encuentran los algoritmos 'Logistic Regression L2', 'Support Vector\n",
    "# Classifier' y 'Random Forest'.\n",
    "\n",
    "# Primero se seleccionan las variables mas importantes \n",
    "X_train_fs, X_test_fs, featEn, posEn = utils.select_features(X_train_enc,\n",
    "                                                             X_train_df, \n",
    "                                                             y_train_os,\n",
    "                                                             X_test_enc, \n",
    "                                                             n = 45)\n",
    "\n",
    "\n",
    "# Se implementan los algoritmos mencionados\n",
    "pipeline_ensemble_soft = Pipeline( [(\"scaler\", StandardScaler()), \\\n",
    "                                        (\"Ensemble_soft\", VotingClassifier(voting = \"soft\",\n",
    "                                                                            weights = [1,0.5,1.75],\n",
    "                                                                            estimators=[\n",
    "                                         (\"logistic\", LogisticRegression(solver = \"saga\",\n",
    "                                                                                    penalty = \"l2\",\n",
    "                                                                                    max_iter = 10000,\n",
    "                                                                                    class_weight = {0: 3, 1: 2},\n",
    "                                                                                    multi_class = \"ovr\",\n",
    "                                                                                    C = 2.91,\n",
    "                                                                                    random_state = 8)),\n",
    "                                       (\"SVC\", SVC(kernel = \"linear\", \n",
    "                                                  class_weight= {0:1, 1:1},\n",
    "                                                                 probability=True,\n",
    "                                                                 decision_function_shape = \"ovr\",\n",
    "                                                                 degree = 2,\n",
    "                                                                 gamma = \"auto\", \n",
    "                                                                 C = 1,\n",
    "                                                                 random_state = 45)),\n",
    "                                        (\"RF\", RandomForestClassifier(max_depth = 3,\n",
    "                                                   criterion = \"log_loss\",\n",
    "                                                   max_features = \"log2\",\n",
    "                                                   oob_score = False,\n",
    "                                                   min_samples_split = 2, # min = 5\n",
    "                                                   class_weight= {0:3, 1:1},\n",
    "                                                   random_state = 45))]))])\n",
    "pipeline_ensemble_soft.fit(X_train_fs, y_train_os)\n",
    "\n",
    "\n",
    "\n",
    "# Ahora viene el código correspondiente a la introducción de datos y predicción del algoritmo\n",
    "\n",
    "# Se lee un archivo .txt con la secuencia de aminoácidos.\n",
    "inputfile=\"aminacids_sequence_original.txt\" \n",
    "f_amin=open(inputfile,\"r\")\n",
    "seq_amin=f_amin.read()\n",
    "#Se reemplazan posibles caracteres \"\\n\" y \"\\r\"\n",
    "seq_amin=seq_amin.replace(\"\\n\",\"\")\n",
    "seq=seq_amin.replace(\"\\r\",\"\")\n",
    "#Se convierte la secuencia en lista, para poder indexar sus elementos\n",
    "seq_amin_list = list(seq_amin)\n",
    "\n",
    "# Se le pide al usuario que introduzca una mutación con el siguiente formato: Aminoácido inicial, posición donde se produce la\n",
    "# mutación, aminoácido final. Los aminoácidos se representan con una única letra y en mayúscula. \n",
    "mut = input(\"Enter your mutation:\")\n",
    "# Se convierte la cadena de entrada en una lista, para poder indexar sus elementos.\n",
    "mut_list = list(mut)\n",
    "\n",
    "# Como los aminoácidos se representan con una única letra, se pueden tomar todos los elementos de la lista menos el primero\n",
    "# y el último para quedarse todo con los dígitos de la posición.\n",
    "mut_pos_list = mut_list[1:-1]\n",
    "\n",
    "# Se unen esos dígtos y se convierten a número entero para poder operar con ellos\n",
    "mut_pos = int(''.join(mut_pos_list))\n",
    "mut_pos = mut_pos - 1\n",
    "\n",
    "#luego vemos que esa mutacion efectivamente puede darse, es decir que en la posicion indicada, origininalmente se encuentra el \n",
    "#aminoácido inicial\n",
    "if mut_list[0] != seq_amin_list[mut_pos]:\n",
    "    print('This is not an existing mutation: the position does not correspond to the initial aminacid')\n",
    "    exit()\n",
    "# Si el aminoácido inicial y el final coinciden, se trata de una mutación idéntica, la cual resultará benigna. Es muy probable\n",
    "# que nadie quiera comprobar este tipo de mutaciones y simplemente se haya errado a la hora de introducir la mutación, por lo\n",
    "# tanto en caso de que esto ocurra saltará un 'warning' que avise al usuario de que la mutación es idéntica.\n",
    "else:\n",
    "    if mut_list[0] == mut_list[-1]:\n",
    "        print('WARNING: This is an identical mutation')\n",
    "\n",
    "# Se vuelven a unir los caracteres para poder seguir con el algoritmo\n",
    "    mut=str(''.join(mut_list))\n",
    "\n",
    "# Se crea un DataFrame con la mutación introducida. Esto es así ya que las funciones que se utilizan a continuación están \n",
    "# definidas de tal manera que toman como entrada un DataFrame\n",
    "    conflictive = pd.DataFrame(columns = [\"Mutationppt\"])\n",
    "    c = pd.DataFrame({'Mutationppt':[mut]})\n",
    "    challenge = pd.concat([conflictive, c])\n",
    "\n",
    "# Se utiliza la función siguiente para generar una tabla con todos los descriptores que después se utilizan en el \n",
    "# algoritmo.\n",
    "    ch_df = utils.KCNQ2_DDBB_generation(challenge)\n",
    "\n",
    "\n",
    "#Quitamos los posibles valores duplicados.\n",
    "#Ahora mismo esto no tiene mucho sentido ya que en el input sólo \n",
    "#podemos introducir las mutaciones de una en una, pero cuando \n",
    "#lo solucione será útil\n",
    "\n",
    "# Check for duplicates \n",
    "#ch_df[ch_df[\"Mutationppt\"].duplicated()]\n",
    "\n",
    "# Remove duplicates if needed\n",
    "#ch_df_clean = ch_df.drop_duplicates(subset = [\"Mutationppt\"])\n",
    "#rest = ch_df.shape[0] -ch_df_clean.shape[0]\n",
    "\n",
    "# Se crea una lista donde aparece únicamente el nombre de la mutación\n",
    "    variants_names = list(ch_df[\"Mutationppt\"])\n",
    "\n",
    "#Se adapta la tabla a los requerimientos del algoritmo\n",
    "    ch_df = utils.preprocessing_ch(ch_df)\n",
    "\n",
    "#Se convierte la tabla a array numérico para que pueda introducirse en el algoritmo\n",
    "    X_ch = ch_df.to_numpy()\n",
    "    X_train_enc, X_ch_enc, X_train_df, X_ch_df = utils.categorical_encoding(X_train_os, X_ch)\n",
    "\n",
    "\n",
    "# Se seleccionan los desciptores más importantes como en el entrenamiento\n",
    "    X_train_fs, X_ch_enc_fs, feat_pred, pos_pred = utils.select_features(X_train_enc,\n",
    "                                                                     X_train_df, \n",
    "                                                                     y_train_os,\n",
    "                                                                     X_ch_enc,\n",
    "                                                                     n = 45)\n",
    "\n",
    "# Se reliza la predicción de la mutación introducida\n",
    "# En la predicción se obtienen dos tipos de resultados. Uno te dice a qué clase pertenece la mutación y el otro la probabilidad \n",
    "# de que la mutación pertenezca a cada clase.\n",
    "    KCNQ2e_y_ch_p = pipeline_ensemble_soft.predict(X_ch_enc_fs)\n",
    "\n",
    "# View probabilities in prediction\n",
    "    KCNQ2eprob = pipeline_ensemble_soft.predict_proba(X_ch_enc_fs)\n",
    "\n",
    "\n",
    "#Se saca por pantalla el resultado de la predicción\n",
    "    if KCNQ2e_y_ch_p == 0:\n",
    "        proba0 = KCNQ2eprob[0,0]*100\n",
    "        print(\"Benign mutation\")\n",
    "        if proba0<=60:\n",
    "            print(\"Success rate: VERY LOW\")\n",
    "        elif 60 < proba0 <=70:\n",
    "            print(\"Success rate: LOW\")\n",
    "        elif 70 < proba0 <=80:\n",
    "            print(\"Success rate: MODERATE\")\n",
    "        elif 80 < proba0 <=90:\n",
    "            print(\"Success rate: HIGH\")\n",
    "        elif 90< proba0:\n",
    "            print(\"Success rate: VERY HIGH\")\n",
    "    else:\n",
    "        proba1 = KCNQ2eprob[0,1]*100\n",
    "        print(\"Pathogenic mutation\")\n",
    "        if proba1<=60:\n",
    "            print(\"Success rate: VERY LOW\")\n",
    "        elif 60 < proba1 <=70:\n",
    "            print(\"Success rate: LOW\")\n",
    "        elif 70 < proba1 <=80:\n",
    "            print(\"Success rate: MODERATE\")\n",
    "        elif 80 < proba1 <=90:\n",
    "            print(\"Success rate: HIGH\")\n",
    "        elif 90< proba1:\n",
    "            print(\"Success rate: VERY HIGH\")\n",
    "#Falta ver qué descriptores queremos que nos imprima por pantalla, cuales se consideran interesantes y cuales no    \n",
    "#ch_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
